{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.random_state = 42\n",
    "import gzip\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread, imresize, imshow\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    '''layer_dims =list holding all units of each layer including input'''\n",
    "\n",
    "    def __init__(self, layer_dims):\n",
    "        '''Initialize all weights and biases\n",
    "        \n",
    "        Parameters =  a dict holding all weights and biases'''\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        self.num_layers = len(layer_dims)#input, hidden, hidden output\n",
    "        self.layer_dims = layer_dims #[3,7,7,1]\n",
    "        self.parameters = {}\n",
    "        \n",
    "        for i in range(1,self.num_layers):\n",
    "            self.parameters['w{}'.format(i)]= np.random.randn(layer_dims[i-1], layer_dims[i])*0.01\n",
    "            self.parameters['b{}'.format(i)] = np.random.randn(1,layer_dims[i])*0.01\n",
    "        \n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \n",
    "        '''feedforward function using sigmoid\n",
    "        for all layers'''\n",
    "        \n",
    "        for i in range(1, self.num_layers):\n",
    "            a =sigmoid(np.dot(a,self.parameters['w'+str(i)])+ self.parameters['b'+str(i)])\n",
    "        return a\n",
    "            \n",
    "    def gather_backprop_data(self, x,y ):\n",
    "        \n",
    "        '''feedforward function used to gather\n",
    "        all the zs (logits) and activations for backpropagation\n",
    "        we also initialize all weight and biase gradients to 0\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        gradients = {}    \n",
    "        for i in range(0,self.num_layers-1):\n",
    "            gradients['w{}'.format(str(i+1))]= np.zeros((self.layer_dims[i], self.layer_dims[i+1]), dtype = np.float32)\n",
    "            gradients['b{}'.format(str(i+1))] = np.zeros((1,self.layer_dims[i+1]),dtype = np.float32)\n",
    "    \n",
    "        activations =[x] \n",
    "        zs = []\n",
    "        a =x\n",
    "        \n",
    "        \n",
    "        for i in range(1, self.num_layers):\n",
    "            z= np.dot(a,self.parameters['w'+str(i)])+ self.parameters['b'+str(i)]\n",
    "            zs.append(z)\n",
    "            a = sigmoid(z)\n",
    "            activations.append(a)\n",
    "            \n",
    "        return gradients, activations, zs\n",
    "    \n",
    "\n",
    "    def calculate_gradients(self,x, y, lambd):\n",
    "        \n",
    "        \n",
    "        '''calculate all gradients with respect to\n",
    "        cost. Here our cost function is cross_entroupy\n",
    "        \n",
    "        last_layer_z_error = dC/dZ  (z is logit)\n",
    "        All weight gradients also include regularazation gradients\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "##### First we calculate the output layer gradients #########\n",
    "        \n",
    "        gradients, activations, zs = self.gather_backprop_data(x,y)\n",
    "        \n",
    "        #gradient of cost with respect to  Z of last layer\n",
    "        last_layer_z_error = ((activations[-1] - y)) \n",
    "        \n",
    "        \n",
    "        \n",
    "        #updating the weight_derivatives of final layer\n",
    "        gradients['w'+ str(self.num_layers -1)] = np.dot(activations[-2].T,last_layer_z_error)/x.shape[0] + (lambd/x.shape[0])*(self.parameters['w'+ str(self.num_layers -1)])\n",
    "        \n",
    "        gradients['b'+ str(self.num_layers -1)] = np.mean(last_layer_z_error, axis =0)\n",
    "        gradients['b'+ str(self.num_layers -1)] = np.expand_dims(gradients['b'+ str(self.num_layers -1)],0)\n",
    "    \n",
    "\n",
    "###HIDDEN LAYER GRADIENTS###\n",
    "\n",
    "        z_previous_layer = last_layer_z_error\n",
    "        \n",
    "        \n",
    "       \n",
    "        for i in reversed(range(1,self.num_layers -1)):\n",
    "            z_previous_layer =np.dot(z_previous_layer,self.parameters['w'+ str(i+1)].T, )*\\\n",
    "                                 (sigmoid_derivative(zs[i-1]))\n",
    "                \n",
    "            gradients['w'+str(i)] = np.dot((activations[i-1].T),z_previous_layer)/x.shape[0] + (lambd/x.shape[0])*(self.parameters['w'+str(i)])\n",
    "            gradients['b'+str(i)] = np.mean(z_previous_layer, axis =0) \n",
    "            gradients['b'+str(i)] = np.expand_dims(gradients['b'+str(i)],0)\n",
    "            \n",
    " \n",
    "        return gradients\n",
    "    \n",
    "    \n",
    "    \n",
    "    def accuracy (self, testing_x, testing_y):\n",
    "        \n",
    "        results =  [(np.argmax(self.feedforward(x)),np.argmax(y))\\\n",
    "                    for x,y in zip(testing_x,testing_y)]\n",
    "        \n",
    "        return results, sum([x==y for x,y in results])/len(results)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        a = (self.feedforward(x))\n",
    "        return np.argmax(a)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def stochastic_gradient_decent(self,training_X, training_Y,testing_X,testing_Y,epochs,\\\n",
    "                                                    splits,learning_rate, lambd):\n",
    "        print('pre training accuracy:', self.accuracy(testing_X,testing_Y)[1])\n",
    "        \n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            indices =np.random.permutation(len(training_X))\n",
    "            training_X = training_X[indices]\n",
    "            training_Y = training_Y[indices]\n",
    "            #random.shuffle(zip(training_X,training_Y))\n",
    "            mini_batches_X, mini_batches_Y =\\\n",
    "                                 create_mini_batches(training_X, training_Y, splits = splits)\n",
    "            for  mini_batch_X, mini_batch_Y in zip(mini_batches_X, mini_batches_Y):\n",
    "               \n",
    "            \n",
    "                    \n",
    "                    \n",
    "                    gradients = self.calculate_gradients(mini_batch_X, mini_batch_Y, lambd)\n",
    "                  \n",
    "                    for i in range(1,self.num_layers):\n",
    "                        \n",
    "                        self.parameters['w'+str(i)] = self.parameters['w'+str(i)]- learning_rate * (gradients['w'+str(i)])\n",
    "                        self.parameters['b'+str(i)] = self.parameters['b'+str(i)] - learning_rate * (gradients['b'+str(i)])\n",
    "                        \n",
    "            \n",
    "            print('post {} epochs training accuracy:'.format(str(epoch+1)), self.accuracy(testing_X,testing_Y)[1])\n",
    "            \n",
    "           \n",
    "            \n",
    "        #print('error is {0:.3f}'.format(error/len(testing_data)))\n",
    "        return self.parameters\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#activation functions\n",
    "\n",
    "def sigmoid(z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "def sigmoid_derivative(z):\n",
    "    \n",
    "    return sigmoid(z) * (1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    f = gzip.open('C:\\\\Users\\\\Moondra\\\\Desktop\\\\Computer Vision\\\\My attempts\\\\data\\\\mnist_expanded.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f)\n",
    "    f.close()  \n",
    "    training_X =training_data[0].reshape(250000, 784)\n",
    "    training_Y =training_data[1].reshape(250000,1)\n",
    "    training_Y = np.eye(10)[training_Y].reshape(250000,10)   #np.eye(n_labels)[target_vector]\n",
    "    test_X = test_data[0].reshape(10000, 784)\n",
    "    test_Y = test_data[1].reshape(10000,1)\n",
    "    test_Y = np.eye(10)[test_Y].reshape(10000,10)\n",
    "    validation_X = validation_data[0]\n",
    "    validation_Y= validation_data[1].reshape(10000,1)\n",
    "    validation_Y = np.eye(10)[validation_Y].reshape(10000,10)\n",
    "    \n",
    "    return (training_X,training_Y ,test_X, test_Y, validation_X,validation_Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Create batches for SGD function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mini_batches(training_X, training_Y, splits = 5000):\n",
    "    \n",
    "    \n",
    "    \n",
    "    mini_batches_training_X = np.split(training_X, 5000)\n",
    "        \n",
    "    mini_batches_training_Y = np.split(training_Y, 5000)\n",
    "    \n",
    "    assert len(mini_batches_training_X) == len(mini_batches_training_Y)\n",
    "                \n",
    "    \n",
    "    return mini_batches_training_X, mini_batches_training_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Run main code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_X,training_Y,test_X, test_Y, VD_X, VD_Y= load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre training accuracy: 0.0982\n",
      "post 1 epochs training accuracy: 0.101\n",
      "post 2 epochs training accuracy: 0.101\n",
      "post 3 epochs training accuracy: 0.1135\n",
      "post 4 epochs training accuracy: 0.1135\n",
      "post 5 epochs training accuracy: 0.0974\n",
      "post 6 epochs training accuracy: 0.1135\n",
      "post 7 epochs training accuracy: 0.1009\n",
      "post 8 epochs training accuracy: 0.1028\n",
      "post 9 epochs training accuracy: 0.0982\n",
      "post 10 epochs training accuracy: 0.1135\n",
      "post 11 epochs training accuracy: 0.1135\n",
      "post 12 epochs training accuracy: 0.1135\n",
      "post 13 epochs training accuracy: 0.1135\n",
      "post 14 epochs training accuracy: 0.1135\n",
      "post 15 epochs training accuracy: 0.1135\n",
      "post 16 epochs training accuracy: 0.1135\n",
      "post 17 epochs training accuracy: 0.1135\n",
      "post 18 epochs training accuracy: 0.1135\n",
      "post 19 epochs training accuracy: 0.1135\n",
      "post 20 epochs training accuracy: 0.1028\n",
      "post 21 epochs training accuracy: 0.1135\n",
      "post 22 epochs training accuracy: 0.1135\n",
      "post 23 epochs training accuracy: 0.1135\n",
      "post 24 epochs training accuracy: 0.101\n",
      "post 25 epochs training accuracy: 0.1135\n",
      "post 26 epochs training accuracy: 0.1135\n",
      "post 27 epochs training accuracy: 0.1135\n",
      "post 28 epochs training accuracy: 0.1135\n",
      "post 29 epochs training accuracy: 0.1135\n",
      "post 30 epochs training accuracy: 0.1135\n",
      "post 31 epochs training accuracy: 0.1135\n",
      "post 32 epochs training accuracy: 0.1135\n",
      "post 33 epochs training accuracy: 0.1135\n",
      "post 34 epochs training accuracy: 0.1135\n",
      "post 35 epochs training accuracy: 0.1135\n",
      "post 36 epochs training accuracy: 0.1135\n",
      "post 37 epochs training accuracy: 0.1135\n",
      "post 38 epochs training accuracy: 0.1028\n",
      "post 39 epochs training accuracy: 0.1135\n",
      "post 40 epochs training accuracy: 0.1135\n",
      "post 41 epochs training accuracy: 0.1135\n",
      "post 42 epochs training accuracy: 0.1135\n",
      "post 43 epochs training accuracy: 0.1135\n",
      "post 44 epochs training accuracy: 0.1135\n",
      "post 45 epochs training accuracy: 0.1135\n",
      "post 46 epochs training accuracy: 0.1135\n",
      "post 47 epochs training accuracy: 0.1135\n",
      "post 48 epochs training accuracy: 0.1135\n",
      "post 49 epochs training accuracy: 0.1135\n",
      "post 50 epochs training accuracy: 0.1135\n",
      "post 51 epochs training accuracy: 0.1135\n",
      "post 52 epochs training accuracy: 0.1135\n",
      "post 53 epochs training accuracy: 0.1135\n",
      "post 54 epochs training accuracy: 0.1135\n",
      "post 55 epochs training accuracy: 0.1135\n",
      "post 56 epochs training accuracy: 0.1135\n",
      "post 57 epochs training accuracy: 0.1135\n",
      "post 58 epochs training accuracy: 0.1135\n",
      "post 59 epochs training accuracy: 0.1135\n",
      "post 60 epochs training accuracy: 0.1032\n",
      "post 61 epochs training accuracy: 0.1135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5de7ba68ce2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mNN\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#input, hidden nodes, output nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnew_parameters\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstochastic_gradient_decent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_Y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_Y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m                                                    \u001b[0msplits\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.50\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#train data, test_data,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m                                                                      \u001b[1;31m#epochs =30, batch_size =10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                                     \u001b[1;31m#learning_rate = 1.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-87ead247515e>\u001b[0m in \u001b[0;36mstochastic_gradient_decent\u001b[1;34m(self, training_X, training_Y, testing_X, testing_Y, epochs, splits, learning_rate, lambd)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m                     \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-87ead247515e>\u001b[0m in \u001b[0;36mcalculate_gradients\u001b[1;34m(self, x, y, lambd)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mz_previous_layer\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_previous_layer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m                                 \u001b[1;33m(\u001b[0m\u001b[0msigmoid_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz_previous_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlambd\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NN =Network([784,60,20, 10]) #input, hidden nodes, output nodes\n",
    "\n",
    "new_parameters= NN.stochastic_gradient_decent(training_X, training_Y,test_X,test_Y,epochs =100,\\\n",
    "                                                    splits =5000,learning_rate =.1, lambd = .50)  #train data, test_data,\n",
    "                                                                     #epochs =30, batch_size =10\n",
    "                                                                    #learning_rate = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scores:\n",
    "\n",
    "1) new_parameters= NN.stochastic_gradient_decent(training_X, training_Y,test_X,test_Y,epochs =100,\\\n",
    "                                                    splits =5000,learning_rate =.01, lambd = .50)\n",
    "\n",
    "\n",
    "**post 57 epochs training accuracy: 0.1135**\n",
    "    \n",
    "    \n",
    "2) new_parameters= NN.stochastic_gradient_decent(training_X, training_Y,test_X,test_Y,epochs =100,\\\n",
    "                                                    splits =5000,learning_rate =.1, lambd = .50)\n",
    "\n",
    "**post 40 epochs training accuracy .88 - .93 -  \n",
    "\n",
    "(fluctuated back and forth for aboutr 20 epochs)\n",
    "\n",
    "\n",
    "3) new_parameters= NN.stochastic_gradient_decent(training_X, training_Y,test_X,test_Y,epochs =100,\\\n",
    "                                                    splits =5000,learning_rate =.1, lambd = .90)\n",
    "\n",
    "**post 20 epochs tranining accuracy  .11 percent steady**\n",
    "                                                    \n",
    "                                                    \n",
    "                                                    \n",
    "4) NN =Network([784,60,20, 10]) #input, hidden nodes, output nodes\n",
    "\n",
    "new_parameters= NN.stochastic_gradient_decent(training_X, training_Y,test_X,test_Y,epochs =100,\\\n",
    "                                                    splits =5000,learning_rate =.1, lambd = 2.0) \n",
    "                                                    \n",
    "                                                    \n",
    "**post 20 epochs tranining accuracy  .11 percent steady**\n",
    "\n",
    "\n",
    "5) NN =Network([784,60,20, 10]) #input, hidden nodes, output nodes\n",
    "\n",
    "new_parameters= NN.stochastic_gradient_decent(training_X, training_Y,test_X,test_Y,epochs =100,\\\n",
    "                                                    splits =5000,learning_rate =1, lambd = .80)\n",
    "                                                    \n",
    "** post 10 epochs training accuracy .11 percent\n",
    "\n",
    "\n",
    "5)NN =Network([784,60,20, 10]) #input, hidden nodes, output nodes\n",
    "\n",
    "new_parameters= NN.stochastic_gradient_decent(training_X, training_Y,test_X,test_Y,epochs =100,\\\n",
    "                                                    splits =5000,learning_rate =.1, lambd = 0)\n",
    "                                                    \n",
    "**post 15 epochs training accuracy = 97%**\n",
    "                                                   \n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN.accuracy(VD_X, VD_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "3+\n",
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
